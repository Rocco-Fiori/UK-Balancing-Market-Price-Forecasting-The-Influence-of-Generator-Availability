{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52c4cf5",
   "metadata": {},
   "source": [
    "# System Price Forecasting - Monthly Rolling Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5701ea",
   "metadata": {},
   "source": [
    "- System price forecasting in the British BM across several short-term time horizons\n",
    "- Comparison of forecasting performance with REMIT generator unavailability data vs without \n",
    "- 2023 data used for model training\n",
    "- 2024 data used for model testing\n",
    "- Rolling monthly window forecasting, with performance averaged across 2024 test data\n",
    "- Previous months data added to next model training set, e.g. After testing on January 2024, January 2024 becomes part of test set for February 2024 and following forecasts \n",
    "- XGBoost regression model with optuna hyperparameter tuning\n",
    "- Model evaluation using MAE, RMSE, R2\n",
    "- Forecast function runs the model, takes one input which is the df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea93d8",
   "metadata": {},
   "source": [
    "## Market Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd25a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import optuna\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline market dataset\n",
    "market_data = pd.read_csv(\"Final_Dataset_Forward_Forecasts_2021_2024.csv\")\n",
    "\n",
    "# Convert 'StartTime' to datetime\n",
    "market_data['StartTime'] = pd.to_datetime(market_data['StartTime'])\n",
    "\n",
    "# Display head of the dataset\n",
    "market_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim data for 2023 onwards\n",
    "market_data = market_data[market_data['StartTime'] >= '2023-01-01']\n",
    "\n",
    "# Reset index\n",
    "market_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop SettlementPeriod column\n",
    "market_data.drop(columns=['SettlementPeriod'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60336070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SystemSellPrice \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(market_data['StartTime'], market_data['SystemPrice'], label='System Sell Price', color='blue')\n",
    "plt.title('UK Balancing Market System Price (2023-2024)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (Â£/MWh)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time of day, day of week, month, and season as features\n",
    "market_data['TimeOfDay'] = market_data['StartTime'].dt.hour\n",
    "market_data['DayOfWeek'] = market_data['StartTime'].dt.dayofweek\n",
    "market_data['Month'] = market_data['StartTime'].dt.month\n",
    "market_data['Season'] = market_data['StartTime'].dt.month % 12 // 3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555feab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude columns\n",
    "columns_to_plot = [col for col in market_data.columns \n",
    "                   if col not in ['StartTime', 'SettlementPeriod', 'SystemPrice']]\n",
    "\n",
    "# Calculate grid size\n",
    "n = len(columns_to_plot)\n",
    "n_cols = 3  # Number of columns in the subplot grid\n",
    "n_rows = math.ceil(n / n_cols)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows), sharex=True)\n",
    "axes = axes.flatten()  # Flatten in case it's a 2D array\n",
    "\n",
    "# Plot each column\n",
    "for i, col in enumerate(columns_to_plot):\n",
    "    axes[i].plot(market_data['StartTime'], market_data[col])\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].grid(True)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Turn off any unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print SysmenPrice statistics\n",
    "print(\"System Price Statistics:\")\n",
    "print(market_data['SystemPrice'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb56b562",
   "metadata": {},
   "source": [
    "## Individual Generator Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e125f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "individual_unit_capacity = pd.read_csv(\"Individual_Unit_Capacity.csv\")\n",
    "individual_unit_capacity_planned = pd.read_csv(\"Individual_Unit_Capacity_Planned.csv\")\n",
    "individual_unit_capacity_unplanned = pd.read_csv(\"Individual_Unit_Capacity_Unplanned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim data for 2023 onwards\n",
    "individual_unit_capacity['StartTime'] = pd.to_datetime(individual_unit_capacity['StartTime'])\n",
    "individual_unit_capacity = individual_unit_capacity[individual_unit_capacity['StartTime'] >= '2023-01-01']\n",
    "\n",
    "individual_unit_capacity_planned['StartTime'] = pd.to_datetime(individual_unit_capacity_planned['StartTime'])\n",
    "individual_unit_capacity_planned = individual_unit_capacity_planned[individual_unit_capacity_planned['StartTime'] >= '2023-01-01']\n",
    "\n",
    "individual_unit_capacity_unplanned['StartTime'] = pd.to_datetime(individual_unit_capacity_unplanned['StartTime'])\n",
    "individual_unit_capacity_unplanned = individual_unit_capacity_unplanned[individual_unit_capacity_unplanned['StartTime'] >= '2023-01-01']\n",
    "\n",
    "# Reset index\n",
    "individual_unit_capacity.reset_index(drop=True, inplace=True)\n",
    "individual_unit_capacity_planned.reset_index(drop=True, inplace=True)\n",
    "individual_unit_capacity_unplanned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge market data with generator data\n",
    "generator_data_merged = individual_unit_capacity.merge(market_data, on='StartTime', how='inner')\n",
    "generator_data_merged_planned = individual_unit_capacity_planned.merge(market_data, on='StartTime', how='inner')\n",
    "generator_data_merged_unplanned = individual_unit_capacity_unplanned.merge(market_data, on='StartTime', how='inner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e502366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Top 50 SHAP features CSV files\n",
    "top_50_fuel = pd.read_csv(\"Top_50_Fuel_Types.csv\")\n",
    "top_50_fuel_planned = pd.read_csv(\"Top_50_Fuel_Types_Planned.csv\")\n",
    "top_50_fuel_unplanned = pd.read_csv(\"Top_50_Fuel_Types_Unplanned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc318b",
   "metadata": {},
   "source": [
    "## Time Horizon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ee2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hour ahead forecast \n",
    "baseline_data_1hr = market_data.copy()\n",
    "\n",
    "# Drop lagged DeratedMargin and LossLossOfLoadProbability columns\n",
    "baseline_data_1hr.drop(columns=['LossOfLoadProbability_2','LossOfLoadProbability_4','LossOfLoadProbability_8','LossOfLoadProbability_12',\n",
    "                          'DeratedMargin_2','DeratedMargin_4','DeratedMargin_8','DeratedMargin_12'], inplace=True)\n",
    "\n",
    "# Create 1 hour lags for SystemPrice, NetImbalanceVolume and InterconnectorFlow\n",
    "baseline_data_1hr['SystemPrice_1hr'] = baseline_data_1hr['SystemPrice'].shift(2)\n",
    "baseline_data_1hr['NetImbalanceVolume_1hr'] = baseline_data_1hr['NetImbalanceVolume'].shift(2)\n",
    "baseline_data_1hr['InterconnectorFlow_1hr'] = baseline_data_1hr['InterconnectorFlow'].shift(2)\n",
    "\n",
    "# Rename DeratedMargin_1 and LossOfLoadProbability_1 columns\n",
    "baseline_data_1hr.rename(columns={'DeratedMargin_1': 'DeratedMargin_1hr', 'LossOfLoadProbability_1': 'LossOfLoadProbability_1hr'}, inplace=True)\n",
    "\n",
    "# Drop non lagged columns\n",
    "baseline_data_1hr.drop(columns=['NetImbalanceVolume', 'InterconnectorFlow'], inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "baseline_data_1hr = baseline_data_1hr.dropna()\n",
    "\n",
    "# Merge baseline data with Generator dataset\n",
    "generator_data_1hr = individual_unit_capacity.merge(baseline_data_1hr, on='StartTime', how='left')\n",
    "generator_data_planned_1hr = individual_unit_capacity_planned.merge(baseline_data_1hr, on='StartTime', how='left')\n",
    "generator_data_unplanned_1hr = individual_unit_capacity_unplanned.merge(baseline_data_1hr, on='StartTime', how='left')\n",
    "\n",
    "# Drop rows with missing values\n",
    "generator_data_1hr = generator_data_1hr.dropna()\n",
    "generator_data_planned_1hr = generator_data_planned_1hr.dropna()\n",
    "generator_data_unplanned_1hr = generator_data_unplanned_1hr.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 hour ahead forecast \n",
    "baseline_data_2hr = market_data.copy()\n",
    "\n",
    "# Drop lagged DeratedMargin and LossLossOfLoadProbability columns\n",
    "baseline_data_2hr.drop(columns=['LossOfLoadProbability_1','LossOfLoadProbability_4','LossOfLoadProbability_8','LossOfLoadProbability_12',\n",
    "                          'DeratedMargin_1','DeratedMargin_4','DeratedMargin_8','DeratedMargin_12'], inplace=True)\n",
    "\n",
    "# Create 1 hour lags for SystemPrice, NetImbalanceVolume and InterconnectorFlow\n",
    "baseline_data_2hr['SystemPrice_2hr'] = baseline_data_2hr['SystemPrice'].shift(4)\n",
    "baseline_data_2hr['NetImbalanceVolume_2hr'] = baseline_data_2hr['NetImbalanceVolume'].shift(4)\n",
    "baseline_data_2hr['InterconnectorFlow_2hr'] = baseline_data_2hr['InterconnectorFlow'].shift(4)\n",
    "\n",
    "# Rename DeratedMargin_2 and LossOfLoadProbability_2 columns\n",
    "baseline_data_2hr.rename(columns={'DeratedMargin_2': 'DeratedMargin_2hr', 'LossOfLoadProbability_2': 'LossOfLoadProbability_2hr'}, inplace=True)\n",
    "\n",
    "# Drop non lagged columns\n",
    "baseline_data_2hr.drop(columns=['NetImbalanceVolume', 'InterconnectorFlow'], inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "baseline_data_2hr = baseline_data_2hr.dropna()\n",
    "\n",
    "# Merge baseline data with Generator dataset\n",
    "generator_data_2hr = individual_unit_capacity.merge(baseline_data_2hr, on='StartTime', how='left')\n",
    "generator_data_planned_2hr = individual_unit_capacity_planned.merge(baseline_data_2hr, on='StartTime', how='left')\n",
    "generator_data_unplanned_2hr = individual_unit_capacity_unplanned.merge(baseline_data_2hr, on='StartTime', how='left')\n",
    "\n",
    "# Drop rows with missing values\n",
    "generator_data_2hr = generator_data_2hr.dropna()\n",
    "generator_data_planned_2hr = generator_data_planned_2hr.dropna()\n",
    "generator_data_unplanned_2hr = generator_data_unplanned_2hr.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c625b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 hour ahead forecast \n",
    "baseline_data_4hr = market_data.copy()\n",
    "\n",
    "# Drop lagged DeratedMargin and LossLossOfLoadProbability columns\n",
    "baseline_data_4hr.drop(columns=['LossOfLoadProbability_1','LossOfLoadProbability_2','LossOfLoadProbability_8','LossOfLoadProbability_12',\n",
    "                          'DeratedMargin_1','DeratedMargin_2','DeratedMargin_8','DeratedMargin_12'], inplace=True)\n",
    "\n",
    "# Create 1 hour lags for SystemPrice, NetImbalanceVolume and InterconnectorFlow\n",
    "baseline_data_4hr['SystemPrice_4hr'] = baseline_data_4hr['SystemPrice'].shift(8)\n",
    "baseline_data_4hr['NetImbalanceVolume_4hr'] = baseline_data_4hr['NetImbalanceVolume'].shift(8)\n",
    "baseline_data_4hr['InterconnectorFlow_4hr'] = baseline_data_4hr['InterconnectorFlow'].shift(8)\n",
    "\n",
    "# Rename DeratedMargin_2 and LossOfLoadProbability_2 columns\n",
    "baseline_data_4hr.rename(columns={'DeratedMargin_4': 'DeratedMargin_4hr', 'LossOfLoadProbability_4': 'LossOfLoadProbability_4hr'}, inplace=True)\n",
    "\n",
    "# Drop non lagged columns\n",
    "baseline_data_4hr.drop(columns=['NetImbalanceVolume', 'InterconnectorFlow'], inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "baseline_data_4hr = baseline_data_4hr.dropna()\n",
    "\n",
    "# Merge baseline data with Generator dataset\n",
    "generator_data_4hr = individual_unit_capacity.merge(baseline_data_4hr, on='StartTime', how='left')\n",
    "generator_data_planned_4hr = individual_unit_capacity_planned.merge(baseline_data_4hr, on='StartTime', how='left')\n",
    "generator_data_unplanned_4hr = individual_unit_capacity_unplanned.merge(baseline_data_4hr, on='StartTime', how='left')\n",
    "\n",
    "# Drop rows with missing values\n",
    "generator_data_4hr = generator_data_4hr.dropna()\n",
    "generator_data_planned_4hr = generator_data_planned_4hr.dropna()\n",
    "generator_data_unplanned_4hr = generator_data_unplanned_4hr.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 hour ahead forecast \n",
    "baseline_data_8hr = market_data.copy()\n",
    "\n",
    "# Drop lagged DeratedMargin and LossLossOfLoadProbability columns\n",
    "baseline_data_8hr.drop(columns=['LossOfLoadProbability_1','LossOfLoadProbability_2','LossOfLoadProbability_4','LossOfLoadProbability_12',\n",
    "                          'DeratedMargin_1','DeratedMargin_2','DeratedMargin_4','DeratedMargin_12'], inplace=True)\n",
    "\n",
    "# Create 1 hour lags for SystemPrice, NetImbalanceVolume and InterconnectorFlow\n",
    "baseline_data_8hr['SystemPrice_8hr'] = baseline_data_8hr['SystemPrice'].shift(16)\n",
    "baseline_data_8hr['NetImbalanceVolume_8hr'] = baseline_data_8hr['NetImbalanceVolume'].shift(16)\n",
    "baseline_data_8hr['InterconnectorFlow_8hr'] = baseline_data_8hr['InterconnectorFlow'].shift(16)\n",
    "\n",
    "# Rename DeratedMargin_2 and LossOfLoadProbability_2 columns\n",
    "baseline_data_8hr.rename(columns={'DeratedMargin_8': 'DeratedMargin_8hr', 'LossOfLoadProbability_8': 'LossOfLoadProbability_8hr'}, inplace=True)\n",
    "\n",
    "# Drop non lagged columns\n",
    "baseline_data_8hr.drop(columns=['NetImbalanceVolume', 'InterconnectorFlow'], inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "baseline_data_8hr = baseline_data_8hr.dropna()\n",
    "\n",
    "# Merge baseline data with Generator dataset\n",
    "generator_data_8hr = individual_unit_capacity.merge(baseline_data_8hr, on='StartTime', how='left')\n",
    "generator_data_planned_8hr = individual_unit_capacity_planned.merge(baseline_data_8hr, on='StartTime', how='left')\n",
    "generator_data_unplanned_8hr = individual_unit_capacity_unplanned.merge(baseline_data_8hr, on='StartTime', how='left')\n",
    "\n",
    "# Drop rows with missing values\n",
    "generator_data_8hr = generator_data_8hr.dropna()\n",
    "generator_data_planned_8hr = generator_data_planned_8hr.dropna()\n",
    "generator_data_unplanned_8hr = generator_data_unplanned_8hr.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 hour ahead forecast \n",
    "baseline_data_12hr = market_data.copy()\n",
    "\n",
    "# Drop lagged DeratedMargin and LossLossOfLoadProbability columns\n",
    "baseline_data_12hr.drop(columns=['LossOfLoadProbability_1','LossOfLoadProbability_2','LossOfLoadProbability_4','LossOfLoadProbability_8',\n",
    "                          'DeratedMargin_1','DeratedMargin_2','DeratedMargin_4','DeratedMargin_8'], inplace=True)\n",
    "\n",
    "# Create 1 hour lags for SystemPrice, NetImbalanceVolume and InterconnectorFlow\n",
    "baseline_data_12hr['SystemPrice_12hr'] = baseline_data_12hr['SystemPrice'].shift(24)\n",
    "baseline_data_12hr['NetImbalanceVolume_12hr'] = baseline_data_12hr['NetImbalanceVolume'].shift(24)\n",
    "baseline_data_12hr['InterconnectorFlow_12hr'] = baseline_data_12hr['InterconnectorFlow'].shift(24)\n",
    "\n",
    "# Rename DeratedMargin_2 and LossOfLoadProbability_2 columns\n",
    "baseline_data_12hr.rename(columns={'DeratedMargin_12': 'DeratedMargin_12hr', 'LossOfLoadProbability_12': 'LossOfLoadProbability_12hr'}, inplace=True)\n",
    "\n",
    "# Drop non lagged columns\n",
    "baseline_data_12hr.drop(columns=['NetImbalanceVolume', 'InterconnectorFlow'], inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "baseline_data_12hr = baseline_data_12hr.dropna()\n",
    "\n",
    "# Merge baseline data with Generator dataset\n",
    "generator_data_12hr = individual_unit_capacity.merge(baseline_data_12hr, on='StartTime', how='left')\n",
    "generator_data_planned_12hr = individual_unit_capacity_planned.merge(baseline_data_12hr, on='StartTime', how='left')\n",
    "generator_data_unplanned_12hr = individual_unit_capacity_unplanned.merge(baseline_data_12hr, on='StartTime', how='left')\n",
    "\n",
    "# Drop rows with missing values\n",
    "generator_data_12hr = generator_data_12hr.dropna()\n",
    "generator_data_planned_12hr = generator_data_planned_12hr.dropna()\n",
    "generator_data_unplanned_12hr = generator_data_unplanned_12hr.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a749cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets of valid generators \n",
    "valid_generators_main      = set(individual_unit_capacity.columns) & set(top_50_fuel['Feature'])\n",
    "valid_generators_planned   = set(individual_unit_capacity.columns) & set(top_50_fuel_planned['Feature'])\n",
    "valid_generators_unplanned = set(individual_unit_capacity.columns) & set(top_50_fuel_unplanned['Feature'])\n",
    "\n",
    "\n",
    "# Generator various datasets: All generator data, planned generator data, unplanned generator data, planned generator top 50, unplanned generator top 50\n",
    "prefixes     = ['generator_data', 'generator_data_planned', 'generator_data_unplanned']\n",
    "time_windows = ['1hr', '2hr', '4hr', '8hr', '12hr']\n",
    "\n",
    "for prefix in prefixes:\n",
    "    if prefix == 'generator_data':\n",
    "        valid_generators = valid_generators_main\n",
    "    elif prefix == 'generator_data_planned':\n",
    "        valid_generators = valid_generators_planned\n",
    "    else:  # 'generator_data_unplanned'\n",
    "        valid_generators = valid_generators_unplanned\n",
    "\n",
    "    for window in time_windows:\n",
    "        name = f\"{prefix}_{window}\"\n",
    "        df = globals()[name]\n",
    "\n",
    "        # preserve market columns (e.g., StartTime, price, etc.)\n",
    "        non_generator_cols = [c for c in df.columns if c not in individual_unit_capacity.columns or c == 'StartTime']\n",
    "\n",
    "        # generator columns to keep for this dataset\n",
    "        generator_cols = sorted(list(valid_generators & set(df.columns)))\n",
    "\n",
    "        # rename rule only for planned/unplanned generator columns\n",
    "        if prefix == 'generator_data_planned':\n",
    "            rename_map = {c: f\"{c}_planned\" for c in generator_cols}\n",
    "        elif prefix == 'generator_data_unplanned':\n",
    "            rename_map = {c: f\"{c}_unplanned\" for c in generator_cols}\n",
    "        else:\n",
    "            rename_map = {}\n",
    "\n",
    "        filtered_cols = generator_cols + non_generator_cols\n",
    "        filtered_df = df[filtered_cols].rename(columns=rename_map)\n",
    "\n",
    "        globals()[f\"{name}_filtered\"] = filtered_df\n",
    "\n",
    "# --- Build combined (planned + unplanned) per horizon ---\n",
    "for window in time_windows:\n",
    "    # base market variables from the non-tagged dataset\n",
    "    base = globals()[f\"generator_data_{window}\"]\n",
    "    market_cols = [c for c in base.columns if c not in individual_unit_capacity.columns or c == 'StartTime']\n",
    "    market_df = base[market_cols].copy()\n",
    "\n",
    "    # get the suffixed, filtered generator sets\n",
    "    planned_df   = globals()[f\"generator_data_planned_{window}_filtered\"]\n",
    "    unplanned_df = globals()[f\"generator_data_unplanned_{window}_filtered\"]\n",
    "\n",
    "    # take only the generator columns from these (avoid duplicating market cols)\n",
    "    planned_gen_cols   = [c for c in planned_df.columns   if c not in market_cols]\n",
    "    unplanned_gen_cols = [c for c in unplanned_df.columns if c not in market_cols]\n",
    "\n",
    "    combined_df = pd.concat(\n",
    "        [market_df, planned_df[planned_gen_cols], unplanned_df[unplanned_gen_cols]],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    globals()[f\"generator_data_planned_unplanned_{window}_filtered\"] = combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86a0fd",
   "metadata": {},
   "source": [
    "## XGBoost Forecasting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabea379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_monthly_rolling_forecast(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = 'SystemPrice',\n",
    "    dt_col: str = 'StartTime',\n",
    "    time_format: str = '%Y-%m-%d %H:%M:%S',\n",
    "    n_trials: int = 100,\n",
    "    random_state: int = 42,\n",
    "    save_model_dir: str = 'models'\n",
    "):\n",
    "    os.makedirs(save_model_dir, exist_ok=True)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[dt_col] = pd.to_datetime(df[dt_col], format=time_format)\n",
    "    df['Month'] = df[dt_col].dt.to_period('M')\n",
    "    \n",
    "    test_months = pd.period_range('2024-01', '2024-12', freq='M')\n",
    "    feature_names = df.drop(columns=[dt_col, target_col, 'Month']).columns.tolist()\n",
    "    \n",
    "    results = []\n",
    "    feature_importances = []\n",
    "\n",
    "    # --- Store full-year actuals and predictions ---\n",
    "    full_year_data = []\n",
    "\n",
    "    for i, month in enumerate(test_months):\n",
    "        print(f\"\\nðŸ”¹ Month: {month}\")\n",
    "        train_data = df[df[dt_col] < month.start_time]\n",
    "        test_data = df[df['Month'] == month]\n",
    "\n",
    "        X_train = train_data.drop(columns=[dt_col, target_col, 'Month'])\n",
    "        y_train = train_data[target_col].values\n",
    "\n",
    "        X_test = test_data.drop(columns=[dt_col, target_col, 'Month'])\n",
    "        y_test = test_data[target_col].values\n",
    "        time_test = test_data[dt_col]\n",
    "\n",
    "        # Optuna tuning\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1.0, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10.0, log=True),\n",
    "                'random_state': random_state,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = XGBRegressor(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_val_pred = model.predict(X_test)\n",
    "            return np.sqrt(mean_squared_error(y_test, y_val_pred))  # return positive RMSE\n",
    "\n",
    "        # âœ… minimize RMSE\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        best_params = study.best_params\n",
    "\n",
    "        model = XGBRegressor(**best_params, random_state=random_state, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Save model\n",
    "        joblib.dump(model, f\"{save_model_dir}/model_{month}.joblib\")\n",
    "\n",
    "        # Metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        results.append({'Month': str(month), 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
    "\n",
    "        # Feature importance\n",
    "        imp = model.feature_importances_\n",
    "        feature_importances.append(imp)\n",
    "\n",
    "        # Store full-year predictions\n",
    "        month_df = pd.DataFrame({'Time': time_test, 'Actual': y_test, 'Forecast': y_pred})\n",
    "        full_year_data.append(month_df)\n",
    "\n",
    "        # Individual plot\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(month_df['Time'], month_df['Actual'], label='Actual', linestyle='--', marker='o', alpha=0.6)\n",
    "        plt.plot(month_df['Time'], month_df['Forecast'], label='Forecast', linestyle='-', marker='x', alpha=0.6)\n",
    "        plt.title(f\"Actual vs Forecast - {month}\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(target_col)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Final full-year plot ---\n",
    "    full_2024_df = pd.concat(full_year_data).sort_values('Time')\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(full_2024_df['Time'], full_2024_df['Actual'], label='Actual', linestyle='--', alpha=0.7)\n",
    "    plt.plot(full_2024_df['Time'], full_2024_df['Forecast'], label='Forecast', alpha=0.7)\n",
    "    plt.title(\"SystemPrice Forecast vs Actual - Full Year 2024\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(target_col)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Results table\n",
    "    results_df = pd.DataFrame(results)\n",
    "    avg_row = {\n",
    "        'Month': 'Average',\n",
    "        'RMSE': results_df['RMSE'].mean(),\n",
    "        'MAE': results_df['MAE'].mean(),\n",
    "        'R2': results_df['R2'].mean()\n",
    "    }\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "    print(\"\\nPerformance Summary:\\n\", results_df)\n",
    "\n",
    "    # Feature importance\n",
    "    feature_df = pd.DataFrame(feature_importances, columns=feature_names)\n",
    "    avg_feature_importance = feature_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    # Plot average feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    avg_feature_importance.plot(kind='barh')\n",
    "    plt.title(\"Average Feature Importances (2024)\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df, feature_df, full_2024_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f34d2",
   "metadata": {},
   "source": [
    "## Exact Forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1285a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baseline, fi_baseline, predictions_baseline = run_monthly_rolling_forecast(market_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_generator_data, fi_generator_data, predictions_generator_data = run_monthly_rolling_forecast(generator_data_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff7bb8",
   "metadata": {},
   "source": [
    "## 1hr Ahead Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf204e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1hr_baseline, fi_1hr_baseline, predictions_1hr_baseline = run_monthly_rolling_forecast(baseline_data_1hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45433b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1hr_generator_data, fi_1hr_generator_data, predictions_1hr_generator_data = run_monthly_rolling_forecast(generator_data_1hr) # This is for all generators (not specific planned/unplanned or top 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf4f37",
   "metadata": {},
   "source": [
    "## 2hr Ahead Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426cccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2hr_baseline, fi_2hr_baseline, predictions_2hr_baseline = run_monthly_rolling_forecast(baseline_data_2hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228798d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2hr_generator_data, fi_2hr_generator_data, predictions_2hr_generator_data = run_monthly_rolling_forecast(generator_data_2hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15bc7b",
   "metadata": {},
   "source": [
    "## 4hr Ahead Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85404a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4hr_baseline, fi_4hr_baseline, predictions_4hr_baseline = run_monthly_rolling_forecast(baseline_data_4hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e897ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4hr_generator_data, fi_4hr_generator_data, predictions_4hr_generator_data = run_monthly_rolling_forecast(generator_data_4hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48929ccb",
   "metadata": {},
   "source": [
    "## 8hr Ahead Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcadd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_8hr_baseline, fi_8hr_baseline, predictions_8hr_baseline = run_monthly_rolling_forecast(baseline_data_8hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d82bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_8hr_generator_data, fi_8hr_generator_data, predictions_8hr_generator_data = run_monthly_rolling_forecast(generator_data_8hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a0a89",
   "metadata": {},
   "source": [
    "## 12hr Ahead Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a00fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_12hr_baseline, fi_12hr_baseline, predictions_12hr_baseline = run_monthly_rolling_forecast(baseline_data_12hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_12hr_generator_data, fi_12hr_generator_data, predictions_12hr_generator_data = run_monthly_rolling_forecast(generator_data_12hr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
